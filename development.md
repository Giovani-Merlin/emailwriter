# Development

This markdown file aims to keep track of each step and the planning of the creation of such a repository, from the research stage of the problem to its implementation in a cloud system.

## TLDR

As I have already spent a lot of time on such a project, and I need to accomplish others tasks as well, I decided to finish such a project in x hours. Also, it's missing further post-processing as avoiding incomplete e-mails (sending it before "endoftext") and choosing the token count.

I ended up investing a lot of time in researching generative text models and even more more in getting a good dataset. Unfortunately it was not possible to segment the emails in a timely manner, so it was not possible to get a dataset that was really suitable for this task.

Using the salutation and the subject it is possible to create a coherent email, however, the model generates the finalization of the email as well (because it is trained to do such action), resulting in an email that does not writes the e-mail specific objective (closing). The solution clearly would be to have a ready-made dataset of just body of emails (or texts that we would like the email to look like) or to be able to segment the e-mails. Besides, it would be better to use an "insert" style models, using context on the left (opening) and final context (closing), but studying such models would take me much more time.

Also, to use a model like pplm it is necessary to change their base code (because the model is loaded in each call of the script). Such a model would be ideal for the creation of emails, we could segment text into categories, such as "business", "positive", "conversation", "scheduling"," etc...And use such categories to guide the text generation (not excluding the ideas before).

Finally, for such a project I just trained a gpt2 template on aeslc dataset customized in the form " subject + special token + prompt/generate text". The results are not satisfactory, but most of the findings and code used in this project serve as a basis for a better email generator.

## Total time

Preparation = 14 hours.

* Research - 8 hours
* Planning - 3 hours
* Data choice/collection - 3 hours

Base development = 22 hours.

* Parsing/data segmentation - 8 hours
* Model usage/exploration - 5 hours
* Training code - 3 hours
* Base api/conteinerization creation - 3 hours
* Readme/documentation - 3 hours
* Cloud service (devops) - X

TOTAL: 36 hours

## Research

## Research

**3 hours**:  Basic understanding of the task, search about generative models, base model's pappers and data sets.

**5 hours**: Deeper exploration

* [GPT-2](http://jalammar.github.io/illustrated-gpt2/) (355M medium, 127M small, 87M small distilled)
* [Decoding Methods](https://huggingface.co/blog/how-to-generate)
* [PPLM](https://eng.uber.com/pplm/) - control technique for gpt2
* [CTRL](https://github.com/salesforce/ctrl) (1.6B)
* [T5](https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45) (60M small, 220 Base)
* [Newer control technique](https://aclanthology.org/2021.findings-emnlp.194.pdf)
* [Simple gpt-2 with discriminator](https://bonkerfield.org/2020/02/combining-gpt-2-and-bert/s)
* [Updated review in control techniques](https://lilianweng.github.io/lil-log/2021/01/02/controllable-neural-text-generation.html)
* [Keywords to teXT, stanford project idea](https://web.stanford.edu/class/cs224n/reports/final_reports/report073.pdf)

### Planning

**3 hours**
Based on [this](https://aclanthology.org/2020.acl-main.108.pdf) paper we can segment the e-mail into salutation, paragraphs, and closing.

#### Salutations and closings

They can be generated by template using as base the analyzed e-mails. We just need to discover the tone of the e-mail and number of people's receipients (this could be done by generation too, but it would increase the processing time and complexity).

Paragraphs are more complicated as they need to be generated by a language model.

#### Paragraphs

"Pre-train" model on emails dataset to get the LGM to follow the email structure, then customize output as:

1. Basic e-mail informations: from, to, subject.
2. Subjectc fields for creating the text
    1. Or a list of subjects to be used as a template of each paragraph (or whole e-mail)
    2. Or a pre-defined list of field to use as template, as:
        * Context (or "init"), main subject, objective.
3. Tone or topic (positive, negative, neutral, comercial, etc)

By 1 we could make specific modes for each person or associate a tone to each person (by using some controlling model or custom tokens. Here we will just use to create the email structure using a template).

In 2 we will try both implementations. 1.1 implementation needs custom tokens/control for each paragraph (thus, harder), 1.2 implementation needs just to use the field as prompt of each paragraph (thus, easier).
T5 model is more flexible and can do all the e-mail directly, GPT-2 needs to divide the e-mail into paragraphs to take in account all the needed fields.

In 3 we will use an objective field to define the tone of the e-mail (in practice, just one more controlling field).

#### Requirements

##### Prototype

No training, just a prototype with a pre-trained model*.

Request text inputs:

1. Salutation, optional
2. Tone
3. Closing, optional
4. From name, optional if closing is present
5. Closing, optional if salutation is present

Request structure inputs:

1. From
2. To
3. Subject

An e-mail can be seen as: "Salutation" + "E-mail body" (from subject) + "Closing". So, we can just map the "default" salutations and closings from one language (english in this project) and use a fine-tuned model to write the body.

##### Main model

Request structure inputs:

1. From
2. To
3. Subject

Request text inputs:

* 1.n - Ordered subjects
* 2 - Tone

Subject would be created from the ordered subjects, closing and salutation would be created from the "To" and "Tone" fields and all the text body by the ordered subjects. Salutation and tone infered from the subjects too.

## Data

Needs to generate text in e-mail like format.

### Base data discover

**2 hours**:

* [Personal mail corpus - stackexchange](https://opendata.stackexchange.com/questions/4517/obtaining-personal-mail-corpus)
* [Webcrawling of open archive email lists](https://github.com/webis-de/acl20-crawling-mailing-lists) - would be the best one, but needs to request access.
* [Git Hub](https://github.com/Mithileysh/Email-Datasets)

#### Enron

Choice of [Enron email dataset](https://www.cs.cmu.edu/~./enron/) for pre-training as it has a significant size ( > 500k emails and 1.4GB) and is more business oriented and has some [libraries](https://github.com/ZhaiResearchGroup/enron-parser) to parse it.
**1 hour**: Choice of the parser. [E-mail segmentation parser](https://aclanthology.org/2020.acl-main.108.pdf),  it's not a easy-to-use neither will output an easy-to-use data, but it would segment the e-mails in "salutations", "paragraph", and "closings" making easy to generate specific datasets.

Parse + analyze data **8 hours**

* Base parser: 2 hours
* Remove duplicates, analyze data and order threads by sending time: 1.5 hours
* Segment each e-mail: 4.5 hours - *Not worked*
  * The code with e-mail segmentation has a problem to use it with the enron dataset (it has a code to parse the enron dataset and use it directly, but the results are inconsistent). Started to fix it but it would take a lot of time - it mix all the e-mails lines together.
  
After further research I dosvered the [aeslc](https://huggingface.co/datasets/aeslc) dataset, which consists in parsed enron emails with the subject...

Finally, I will use the e-mails in their "aeslc" form directly, finetuning the LGM to write e-mails, including the salutation and closing...I woul like to enphatise that segmenting the e-mails would be the best way to do it.

Group, labelize:

### Model

#### Processes

**2 hours**
gpt2/gpt-neo: Append labels and generate text after. Can use salutation, subject, closing as template and the tone too. (but this increases considerably the initial size and processing time, maybe better to break it in independtly parts)
T5: Can create specific task like "Text about {subject}:" and the do like summarization.
PPLM: "Equal" as gpt2 but uses a small model (already trained or trained by creating an aritificial dataset by classifying the e-mails using a large text classifier model). Hard to use for any subject, maybe use subject as input and small model to define only the tone.

#### Training

[optimize gpt-2 and t5 for nvidia](https://developer.nvidia.com/blog/optimizing-t5-and-gpt-2-for-real-time-inference-with-tensorrt/)
